# Synthea Delta Lake Pipeline â€“ Demo Project

This project demonstrates how to build a **Delta Lake** pipeline with **PySpark** on top of synthetic healthcare data generated by [Synthea](https://synthea.mitre.org/downloads). It extends a basic ETL workflow into a modern Lakehouse pattern by showcasing Delta Lake capabilities such as ACID transactions, time travel, and incremental updates.

---

## ğŸ“Š Project Overview

**Goal**: Showcase Delta Lake as an enhancement to Spark pipelines for reliable, versioned, and auditable data storage.

**Key Features**:

- Initialize Spark with Delta Lake support.
- Load synthetic healthcare data from CSV (patients, encounters, conditions, medications, observations).
- Clean, aggregate, and join data into a patient-level table.
- Persist curated data in Delta format instead of plain Parquet.
- Demonstrate Delta Lake advanced capabilities:
    - ğŸ”’ ACID transactions â€“ reliable overwrites and inserts.
    - âª Time travel â€“ query historical versions of the table.

---

## ğŸ“ Project Structure

[`data/raw`](data/raw) â€“ Raw CSVs generated by [Synthea](https://mitre.box.com/shared/static/aw9po06ypfb9hrau4jamtvtz0e5ziucz.zip)

[`data/clean`](data/clean) â€“ Cleaned Delta tables (e.g., [patients_main](data/clean/patients_main/))

[`notebooks`](notebooks) â€“ Jupyter notebooks for:
- [Exploration & transformation](notebooks/exploration_and_transformation.ipynb)
- [Delta Lake Showcase (SQL exploration)](notebooks/delta_lake_showcase.ipynb)
- [Delta Lake ACID features](notebooks/delta_lake_features_acid.ipynb)
- [Delta Lake Time Travel](notebooks/delta_lake_features_tt.ipynb)

---

## ğŸš€ Step-by-step Overview

1. **ğŸ”¥ Initialize Spark with Delta Extensions**  
   Configure Spark session with `io.delta.sql.DeltaSparkSessionExtension` and `DeltaCatalog`.

2. **ğŸ—‚ï¸ Load Healthcare Data**  
   Import raw patient, encounter, condition, medication, and observation CSVs.

3. **ğŸ§¹ Transformations & Aggregations**  
   Clean missing values, compute aggregates, and join into a single `patients_main` DataFrame.

4. **ğŸ’¾ Save as Delta Table**  
   Store curated patient-level data in Delta format (`data/clean/patients_main`).

5. **ğŸ“Š Delta Lake SQL Showcase**  
   Register Delta tables as temporary views and run SQL queries directly with `spark.sql()`.

6. **ğŸ”’ ACID Transactions Demo**  
   Safely overwrite table with an updated DataFrame.  
   Guarantee atomic, consistent, isolated, and durable writes.

7. **âª Time Travel Demo**  
   Query historical versions of the table using `versionAsOf`.  
   Compare row counts and changes across versions.

---

## ğŸ“ Notes

**Delta Lake** provides transactional reliability and data versioning, making pipelines more robust than plain **Spark** + **Parquet**.

**Time travel** enables auditing and debugging, while merges simplify incremental data ingestion.

This project builds directly on top of the [Synthea ETL with PySpark](https://github.com/armandaslid/synthea-etl-pyspark) project, demonstrating how a basic ETL pipeline can evolve into a Lakehouse architecture with Delta Lake.

ğŸ‘‰ Delta Lake also supports more advanced features, such as:
- **Schema evolution & enforcement** â€“ handle changes in data structure safely.  
- **Upserts (MERGE INTO)** â€“ simplify incremental data ingestion.  
- **Z-ordering** â€“ optimize query performance on large datasets.  
- **Streaming reads & writes** â€“ unify batch and streaming data pipelines.  
