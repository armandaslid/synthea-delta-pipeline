# Synthea Delta Lake Pipeline – Demo Project

This project demonstrates how to build a **Delta Lake** pipeline with **PySpark** on top of synthetic healthcare data generated by [Synthea](https://synthea.mitre.org/downloads). It extends a basic ETL workflow into a modern Lakehouse pattern by showcasing Delta Lake capabilities such as ACID transactions, time travel, and incremental updates.

---

## 📊 Project Overview

**Goal**: Showcase Delta Lake as an enhancement to Spark pipelines for reliable, versioned, and auditable data storage.

**Key Features**:

- Initialize Spark with Delta Lake support.
- Load synthetic healthcare data from CSV (patients, encounters, conditions, medications, observations).
- Clean, aggregate, and join data into a patient-level table.
- Persist curated data in Delta format instead of plain Parquet.
- Demonstrate Delta Lake advanced capabilities:
    - 🔒 ACID transactions – reliable overwrites and inserts.
    - ⏪ Time travel – query historical versions of the table.

---

## 📁 Project Structure

[`data/raw`](data/raw) – Raw CSVs generated by [Synthea](https://mitre.box.com/shared/static/aw9po06ypfb9hrau4jamtvtz0e5ziucz.zip)

[`data/clean`](data/clean) – Cleaned Delta tables (e.g., [patients_main](data/clean/patients_main/))

[`notebooks`](notebooks) – Jupyter notebooks for:
- [Exploration & transformation](notebooks/exploration_and_transformation.ipynb)
- [Delta Lake Showcase (SQL exploration)](notebooks/delta_lake_showcase.ipynb)
- [Delta Lake ACID features](notebooks/delta_lake_features_acid.ipynb)
- [Delta Lake Time Travel](notebooks/delta_lake_features_tt.ipynb)

---

## 🚀 Step-by-step Overview

1. **🔥 Initialize Spark with Delta Extensions**  
   Configure Spark session with `io.delta.sql.DeltaSparkSessionExtension` and `DeltaCatalog`.

2. **🗂️ Load Healthcare Data**  
   Import raw patient, encounter, condition, medication, and observation CSVs.

3. **🧹 Transformations & Aggregations**  
   Clean missing values, compute aggregates, and join into a single `patients_main` DataFrame.

4. **💾 Save as Delta Table**  
   Store curated patient-level data in Delta format (`data/clean/patients_main`).

5. **📊 Delta Lake SQL Showcase**  
   Register Delta tables as temporary views and run SQL queries directly with `spark.sql()`.

6. **🔒 ACID Transactions Demo**  
   Safely overwrite table with an updated DataFrame.  
   Guarantee atomic, consistent, isolated, and durable writes.

7. **⏪ Time Travel Demo**  
   Query historical versions of the table using `versionAsOf`.  
   Compare row counts and changes across versions.

---

## 📝 Notes

**Delta Lake** provides transactional reliability and data versioning, making pipelines more robust than plain **Spark** + **Parquet**.

**Time travel** enables auditing and debugging, while merges simplify incremental data ingestion.

This project builds directly on top of the [Synthea ETL with PySpark](https://github.com/armandaslid/synthea-etl-pyspark) project, demonstrating how a basic ETL pipeline can evolve into a Lakehouse architecture with Delta Lake.

👉 Delta Lake also supports more advanced features, such as:
- **Schema evolution & enforcement** – handle changes in data structure safely.  
- **Upserts (MERGE INTO)** – simplify incremental data ingestion.  
- **Z-ordering** – optimize query performance on large datasets.  
- **Streaming reads & writes** – unify batch and streaming data pipelines.  
